loss = "mae")
library(tensorflow)
## naive method - predict as the last temperature (24 hours ago)
evaluate_naive_method <- function() {
batch_maes <- c()
for (step in 1:val_steps) {
c(samples, targets) %<-% val_gen()
preds <- samples[,dim(samples)[[2]],2]
mae <- mean(abs(preds - targets))
batch_maes <- c(batch_maes, mae)
}
print(mean(batch_maes))
}
mean_batch_mae <- evaluate_naive_method()
library(keras)
mean_batch_mae <- evaluate_naive_method()
celsius_mae <- mean_batch_mae * std[[2]]
## simple dense layers
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mae")
?fit_generator
dm(data)
dim(data)
history <- model %>% fit_generator(train_gen,
steps_per_epoch = 500,
epochs = 1,
validation_data = val_gen,
validation_steps = val_steps)
plot(history)
history <- model %>% fit_generator(train_gen,
steps_per_epoch = 500,
epochs = 5,
validation_data = val_gen,
validation_steps = val_steps)
plot(history)
## layer_gru
model <- keras_model_sequential() %>%
layer_gru(units = 32, input_shape = list(NULL, dim(data)[[-1]])) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
loss = "mae")
history2 <- model %>% fit_generator(train_gen,
steps_per_epoch = 500,
epochs = 5,
validation_data = val_gen,
validation_steps = val_steps)
plot(history2)
model <- keras_model_sequential() %>%
layer_gru(units = 32, dropout = 0.2, recurrent_dropout = 0.2,input_shape = list(NULL, dim(data)[[-1]])) %>%
layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(), loss = "mae")
history3 <- model %>% fit_generator(train_gen,
steps_per_epoch = 500,
epochs = 5,
validation_data = val_gen,
validation_steps = val_steps)
plot(history3)
## 1D convnet + gru
step <- 3
lookback <- 720
delay <- 144
train_gen <- generator(data,
lookback = lookback,
delay = delay,
min_index = 1,
max_index = 200000,
shuffle = TRUE,
step = step)
val_gen <- generator(data,
lookback = lookback,
delay = delay,
min_index = 200001,
max_index = 300000,
step = step)
test_gen <- generator(data,
lookback = lookback,
delay = delay,
min_index = 300001,
max_index = NULL,
step = step)
val_steps <- (300000 - 200001 - lookback) / 128
test_steps <- (nrow(data) - 300001 - lookback) / 128
model <- keras_model_sequential() %>%
layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu", input_shape = list(NULL, dim(data)[[-1]])) %>%
layer_max_pooling_1d(pool_size = 3) %>%
layer_conv_1d(filters = 32, kernel_size = 5, activation = "relu") %>%
layer_gru(units = 32, dropout = 0.1, recurrent_dropout = 0.5) %>%
layer_dense(units = 1)
summary(model)
model %>% compile(optimizer = optimizer_rmsprop(), loss = "mae")
history6 <- model %>% fit_generator(train_gen,
steps_per_epoch = 500,
epochs = 5,
validation_data = val_gen,
validation_steps = val_steps)
library(keras)
input_tensor <- layer_input(shape = c(64))
output_tensor <- input_tensor %>% layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>% layer_dense(units = 10, activation = "softmax")
model <- keras_model(input_tensor, output_tensor)
summary(model)
model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy")
x_train <- array(runif(1000 * 64), dim = c(1000, 64))
y_train <- array(runif(1000 * 10), dim = c(1000, 10))
model %>% fit(x_train, y_train, epochs = 10, batch_size = 128)
model %>% evaluate(x_train, y_train)
library(keras)
text_vocabulary_size <- 10000
ques_vocabulary_size <- 10000
answer_vocabulary_size <- 500
text_input <- layer_input(shape = list(NULL), dtype = "int32", name = "text")
encoded_text <- text_input %>%
layer_embedding(input_dim = 64, output_dim = text_vocabulary_size) %>%
layer_lstm(units = 32)
question_input <- layer_input(shape = list(NULL), dtype = "int32", name = "question")
encoded_question <- question_input %>%
layer_embedding(input_dim = 32, output_dim = ques_vocabulary_size) %>%
layer_lstm(units = 16)
concatenated <- layer_concatenate(list(encoded_text, encoded_question))
answer <- concatenated %>% layer_dense(units = answer_vocabulary_size, activation = "softmax")
model <- keras_model(list(text_input, question_input), answer)
model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy", metrics = c("acc"))
num_samples <- 1000
max_length <- 100
random_matrix <- function(range, nrow, ncol) {
matrix(sample(range, size = nrow * ncol, replace = TRUE), nrow = nrow, ncol = ncol)
}
text <- random_matrix(1:text_vocabulary_size, num_samples, max_length)
question <- random_matrix(1:ques_vocabulary_size, num_samples, max_length)
answers <- random_matrix(0:1, num_samples, answer_vocabulary_size)
#model %>% fit(list(text, question), answers, epochs = 10, batch_size = 128)
model %>% fit(list(text = text, question = question), answers, epochs = 10, batch_size = 128 )
model %>% fit(list(text, question), answers, epochs = 10, batch_size = 128)
model %>% fit(list(text, question), answers, epochs = 10, batch_size = 128)
text <- random_matrix(1:text_vocabulary_size, num_samples, max_length)
question <- random_matrix(1:ques_vocabulary_size, num_samples, max_length)
answers <- random_matrix(0:1, num_samples, answer_vocabulary_size)
model %>% fit(list(text, question), answers, epochs = 10, batch_size = 128)
model %>% fit(list(text = text, question = question), answers, epochs = 10, batch_size = 128 )
text <- random_matrix(1:text_vocabulary_size, num_samples, max_length)
question <- random_matrix(1:ques_vocabulary_size, num_samples, max_length)
answers <- random_matrix(0:1, num_samples, answer_vocabulary_size)
#model %>% fit(list(text, question), answers, epochs = 10, batch_size = 128)
model %>% fit(list(text = text, question = question), answers, epochs = 10, batch_size = 128 )
encoded_question <- question_input %>%
layer_embedding(input_dim = 64, output_dim = ques_vocabulary_size) %>%
layer_lstm(units = 32)
concatenated <- layer_concatenate(list(encoded_text, encoded_question))
answer <- concatenated %>% layer_dense(units = answer_vocabulary_size, activation = "softmax")
model <- keras_model(list(text_input, question_input), answer)
model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy", metrics = c("acc"))
num_samples <- 1000
max_length <- 100
random_matrix <- function(range, nrow, ncol) {
matrix(sample(range, size = nrow * ncol, replace = TRUE), nrow = nrow, ncol = ncol)
}
text <- random_matrix(1:text_vocabulary_size, num_samples, max_length)
question <- random_matrix(1:ques_vocabulary_size, num_samples, max_length)
answers <- random_matrix(0:1, num_samples, answer_vocabulary_size)
#model %>% fit(list(text, question), answers, epochs = 10, batch_size = 128)
model %>% fit(list(text = text, question = question), answers, epochs = 10, batch_size = 128 )
?layer_embedding
text_input <- layer_input(shape = list(NULL), dtype = "int32", name = "text")
encoded_text <- text_input %>%
layer_embedding(input_dim = text_vocabulary_size, output_dim = 64) %>%
layer_lstm(units = 32)
question_input <- layer_input(shape = list(NULL), dtype = "int32", name = "question")
encoded_question <- question_input %>%
layer_embedding(input_dim = ques_vocabulary_size, output_dim = 32) %>%
layer_lstm(units = 16)
concatenated <- layer_concatenate(list(encoded_text, encoded_question))
answer <- concatenated %>% layer_dense(units = answer_vocabulary_size, activation = "softmax")
model <- keras_model(list(text_input, question_input), answer)
model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy", metrics = c("acc"))
num_samples <- 1000
max_length <- 100
random_matrix <- function(range, nrow, ncol) {
matrix(sample(range, size = nrow * ncol, replace = TRUE), nrow = nrow, ncol = ncol)
}
text <- random_matrix(1:text_vocabulary_size, num_samples, max_length)
question <- random_matrix(1:ques_vocabulary_size, num_samples, max_length)
answers <- random_matrix(0:1, num_samples, answer_vocabulary_size)
#model %>% fit(list(text, question), answers, epochs = 10, batch_size = 128)
model %>% fit(list(text = text, question = question), answers, epochs = 10, batch_size = 128 )
text_input <- layer_input(shape = list(NULL), dtype = "int32", name = "text")
encoded_text <- text_input %>%
layer_embedding(input_dim = text_vocabulary_size + 1, output_dim = 64) %>%
layer_lstm(units = 32)
question_input <- layer_input(shape = list(NULL), dtype = "int32", name = "question")
encoded_question <- question_input %>%
layer_embedding(input_dim = ques_vocabulary_size + 1, output_dim = 32) %>%
layer_lstm(units = 16)
concatenated <- layer_concatenate(list(encoded_text, encoded_question))
answer <- concatenated %>% layer_dense(units = answer_vocabulary_size, activation = "softmax")
model <- keras_model(list(text_input, question_input), answer)
model %>% compile(optimizer = "rmsprop", loss = "categorical_crossentropy", metrics = c("acc"))
num_samples <- 1000
max_length <- 100
random_matrix <- function(range, nrow, ncol) {
matrix(sample(range, size = nrow * ncol, replace = TRUE), nrow = nrow, ncol = ncol)
}
text <- random_matrix(1:text_vocabulary_size, num_samples, max_length)
question <- random_matrix(1:ques_vocabulary_size, num_samples, max_length)
answers <- random_matrix(0:1, num_samples, answer_vocabulary_size)
#model %>% fit(list(text, question), answers, epochs = 10, batch_size = 128)
model %>% fit(list(text = text, question = question), answers, epochs = 10, batch_size = 128)
1000/1.25
1200/1.25
900*1.25
800*1.25
700*1.25
750*1.25
1400+833+938
1400+833+875
800/1.25
700*1.25
1200*1.25
1000*1.25
1100*1.25
1000/7.5
160*8
install.packages("pcalg")
#install.packages("pcalg")
library(pcalg)
version
#install.packages("pcalg")
install.packages("graph")
update.packages()
.9*.72+.1*.9
.4*.35+.5*.4+.1*.9
.5*(-7)+.3*5+.2*20
.15*300+.7*50+.1*(-400)+.05*(-500)
472-48
472-10
10/22
125+33
158*4
632-60
572-165
9/24
1/15
3/7
208725/3400000
64063/1400000
17307/208725
30946/17307
821+2758
19900*1.2
82245/1000000
42527/430000
11538/82245
192295/11538
19295/11538
5000*1.2
4431+3407
28501/320000
25136/110000
3412/28501
6438/3412
6400*1.2
989+342
254+137
36891/5*1.2
4400+525*7.5
3500*.76
4500+2660
4500*.76
4500+3420
24*7.5
27/200
270/2270
110*97/2270
110*97/2270^2
100/12
100/12*9
100/12*8
7/12
8/12
8/129/12
8/12
9/12
library(bayesm)
setwd("C:/OnlineSync/Mega/R/work/maxdiff/test")
knitr::opts_chunk$set(echo = FALSE)
source("surveyconfig.R")
designctx = readRDS(designctxfile)
cbc.df = read.csv(answersfile, header = TRUE, sep = ",", quote = "\"", stringsAsFactors = FALSE)
simcoefs = NULL
if (file.exists(simulatecoefsfile)) simcoefs = readRDS(simulatecoefsfile)
r = lapply(colnames(designctx$survey), function(cn) {
if (!is.null(cbc.df[[cn]])) {
cbc.df[[cn]] <<- factor(cbc.df[[cn]], levels = levels(designctx$survey[[cn]]))
}
})
r = lapply(colnames(designctx$fullfact_covdesign), function(cn) {
if (!is.null(cbc.df[[cn]])) {
cbc.df[[cn]] <<- factor(cbc.df[[cn]], levels = levels(designctx$fullfact_covdesign[[cn]]))
# ovo dodajemo da izbjegnemo kombinaciju (0, 0) koja radi problem recimo za LCA: softmax(0, 0)
# uvijek daje jednake vjerojatnosti za sve segmente pa ne možemo dobiti ništa osim toga
contrasts(cbc.df[[cn]]) <<- contr.sum
}
})
#if (!is.null(cbc.df$questionnaire.id)) cbc.df$questionnaire.id = NULL # maknemo questionnaire.id jer nam ne treba
print(designctx$items)
bw_encode_vectors = function(v1, v2, nalts) {
if (length(v1) != length(v2)) stop("invalid vectors' lengths")
if ((length(v1) %% nalts) != 0) stop("invalid vectors' lengths vs. number of alternatives")
v_coded = rep(0, 2*length(v1))
for (q in 1:(length(v1)/nalts)) {
i_start = (q-1)*nalts+1
i_end = q*nalts
v_coded[(q-1)*nalts+(i_start:i_end)] = v1[i_start:i_end]
v_coded[q*nalts+(i_start:i_end)] = v2[i_start:i_end]
}
v_coded
}
bw_encode_dfs = function(v1, v2, nalts) {
if ((nrow(v1) != nrow(v2)) || (ncol(v1) != ncol(v2))) stop("invalid data frames' dimensions")
if ((nrow(v1) %% nalts) != 0) stop("invalid data frames' dimensions vs. number of alternatives")
v_coded = getdfcolumns(v1[rep(1:nrow(v1), 2), ], cnames = colnames(v1))
for (q in 1:(nrow(v1)/nalts)) {
i_start = (q-1)*nalts+1
i_end = q*nalts
v_coded[(q-1)*nalts+(i_start:i_end), ] = v1[i_start:i_end, ]
v_coded[q*nalts+(i_start:i_end), ] = v2[i_start:i_end, ]
}
v_coded
}
bw_encode_data = function(v1, v2, nalts) {
if (is.vector(v1) && is.vector(v2)) {
bw_encode_vectors(v1, v2, nalts)
} else if (is.data.frame(v1) && is.data.frame(v2)) {
bw_encode_dfs(v1, v2, nalts)
} else if (is.matrix(v1) && is.matrix(v2)) {
bw_encode_dfs(v1, v2, nalts)
} else {
stop("invalid input to bw_encode_data")
}
}
bw_encode_data_1 = function(v1, nalts) {
bw_encode_data(v1, v1, nalts)
}
bw_decode_vectors = function(v_coded, nalts) {
if ((length(v_coded) %% (2*nalts)) != 0) stop("invalid vector length vs. number of alternatives")
v = array(0, dim = c(length(v_coded)/2, 2))
for (q in 1:(length(v_coded)/(2*nalts))) {
i_start = (q-1)*(2*nalts)+1
i_end = q*(2*nalts)
j_start = i_start-(q-1)*nalts
j_end = (i_start+nalts-1)-(q-1)*nalts
v[j_start:j_end, 1] = v_coded[i_start:(i_start+nalts-1)]
v[j_start:j_end, 2] = v_coded[(i_start+nalts):i_end]
}
v
}
bw_decode_dfs = function(v_coded, nalts) {
if ((nrow(v_coded) %% (2*nalts)) != 0) stop("invalid data frame dimensions vs. number of alternatives")
v = array(0, dim = c(length(v_coded)/2, 2))
v1 = v2 = getdfcolumns(v_coded[1:(nrow(v_coded)/2), ], cnames = colnames(v_coded))
for (q in 1:(nrow(v_coded)/(2*nalts))) {
i_start = (q-1)*(2*nalts)+1
i_end = q*(2*nalts)
j_start = i_start-(q-1)*nalts
j_end = (i_start+nalts-1)-(q-1)*nalts
v1[j_start:j_end, ] = v_coded[i_start:(i_start+nalts-1), ]
v2[j_start:j_end, ] = v_coded[(i_start+nalts):i_end, ]
}
list(v1, v2)
}
bw_decode_data = function(v_coded, nalts) {
if (is.vector(v_coded)) {
bw_decode_vectors(v_coded, nalts)
} else if (is.data.frame(v_coded)) {
bw_decode_dfs(v_coded, nalts)
} else if (is.matrix(v_coded)) {
bw_decode_dfs(v_coded, nalts)
} else {
stop("invalid input to bw_decode_data")
}
}
# test.df = cbc.df[1:9, ]
#
# bw_enc = bw_encode_data(test.df$best_choice, test.df$worst_choice, designctx$nalternatives)
# bw_dec = bw_decode_data(bw_enc, designctx$nalternatives)
# any(bw_dec[, 1] != test.df$best_choice)
# any(bw_dec[, 2] != test.df$worst_choice)
#
# bw_enc = bw_encode_data(test.df, test.df, designctx$nalternatives)
# bw_dec = bw_decode_data(bw_enc, designctx$nalternatives)
# any(bw_dec[[1]] != test.df)
# any(bw_dec[[2]] != test.df)
# domodelmatrix = function(alt, nalts, covs = NULL, bwencode = TRUE) {
#   ret_matrix = encode_items(alt) # best choice encoded
#   items.columns = colnames(ret_matrix)
#   if (bwencode) ret_matrix = bw_encode_data(ret_matrix, (-1)*ret_matrix, nalts)
#   cov.columns = c()
#   if (!is.null(covs)) {
#     covs_encoded = encode_covariates(covs)
#     cov.columns = colnames(covs_encoded)
#     if (bwencode) covs_encoded = bw_encode_data(covs_encoded, (-1)*covs_encoded, nalts)
#     ret_matrix = cbind(ret_matrix, covs_encoded)
#   }
#   attr(ret_matrix, "items.columns") = items.columns
#   attr(ret_matrix, "cov.columns") = cov.columns
#   ret_matrix
# }
domodelmatrix_itemsonly = function(alt) {
ret_matrix = encode_items(alt) # best choice encoded
items.columns = colnames(ret_matrix)
attr(ret_matrix, "items.columns") = items.columns
ret_matrix
}
domodelmatrix_itemscovs = function(alt, covs) {
ret_matrix = domodelmatrix_itemsonly(alt)
items.columns = attr(ret_matrix, "items.columns")
cov.columns = c()
covs_encoded = encode_covariates(covs)
cov.columns = colnames(covs_encoded)
ret_matrix = cbind(ret_matrix, covs_encoded)
attr(ret_matrix, "items.columns") = items.columns
attr(ret_matrix, "cov.columns") = cov.columns
ret_matrix
}
domodelmatrix_itemsonly_bwencode = function(alt, nalts) {
ret_matrix = domodelmatrix_itemsonly(alt)
items.columns = attr(ret_matrix, "items.columns")
ret_matrix = bw_encode_data(ret_matrix, (-1)*ret_matrix, nalts)
attr(ret_matrix, "items.columns") = items.columns
ret_matrix
}
domodelmatrix_itemscovs_bwencode = function(alt, covs, nalts) {
ret_matrix = domodelmatrix_itemscovs(alt, covs)
items.columns = attr(ret_matrix, "items.columns")
cov.columns =   attr(ret_matrix, "cov.columns")
ret_matrix = bw_encode_data(ret_matrix, (-1)*ret_matrix, nalts)
attr(ret_matrix, "items.columns") = items.columns
attr(ret_matrix, "cov.columns") = cov.columns
ret_matrix
}
# petlja za svaki resp.id
# enkodirati pitanja, covariates, best+worst choice
# ako ima, enkodirati anchor pitanja kao choice, dodati na kraj data.framea za resp.id
cbc.mm = data.frame(NULL)
for (resp.id in 1:max(cbc.df$resp.id)) {
# izvuci subframe
cbc.df.sf = cbc.df[cbc.df$resp.id == resp.id, ]
nalts = designctx$nalternatives
covs = getdfcolumns(cbc.df.sf, cnames = names(designctx$covariates))
sf.mm = domodelmatrix_itemscovs_bwencode(cbc.df.sf$alt, covs, nalts)
items.columns = attr(sf.mm, "items.columns")
cov.columns = attr(sf.mm, "cov.columns")
cbc.mm.sf = data.frame(cbind(resp.id = bw_encode_data_1(cbc.df.sf$resp.id, nalts),
ques = rep(1:(2*max(cbc.df.sf$ques)), each = nalts),
#alt = cbc.df$alt,
alt = rep(1:nalts, nrow(sf.mm)/nalts),
sf.mm,
choice = bw_encode_data(cbc.df.sf$best_choice, cbc.df.sf$worst_choice, nalts)))
if (designctx$anchors > 0) {
anchorcolnames = getanchorcolnames(designctx$anchors)
anchorcols = cbc.df.sf[, colnames(cbc.df.sf) %in% anchorcolnames]
best_choices = cbc.df.sf[cbc.df$best_choice == 1, ]$alt
worst_choices = cbc.df.sf[cbc.df$worst_choice == 1, ]$alt
anchors = getanchors(designctx$anchors, best_choices, worst_choices)
anch.mm = domodelmatrix_itemscovs(anchors, getdfcolumns(covs[rep(1, length(anchors)), ], cnames = colnames(covs)))
anch.mm = anch.mm[rep(1:nrow(anch.mm), each = 2), ]
anch.mm[2*(1:(nrow(anch.mm)/2)), items.columns] = 0
anch.choices = rep(t(anchorcols[1, ]), each = 2)
anch.choices[2*(1:(length(anch.choices)/2))-1][anchorcols[1, ] == 2] = 0 # neparni
anch.choices[2*(1:(length(anch.choices)/2))][anchorcols[1, ] == 1] = 0 # parni
anch.choices[2*(1:(length(anch.choices)/2))][anchorcols[1, ] == 2] = 1 # parni
anch.mm.sf = data.frame(cbind(resp.id = rep(resp.id, nrow(anch.mm)),
ques = rep(max(cbc.mm.sf$ques+1):(max(cbc.mm.sf$ques)+length(anchors)), each = 2),
alt = rep(1:2, nrow(anch.mm)/2),
anch.mm,
choice = anch.choices))
cbc.mm.sf = rbind(cbc.mm.sf, anch.mm.sf)
}
cbc.mm = rbind(cbc.mm, cbc.mm.sf)
}
rownames(cbc.mm) = c()
if (designctx$anchors == 0) {
# remove the Intercept column - it is essentially contained in the alt column (Intercept is when all predictors are 0 - that leaves only the alt as the predictor)
# when we use anchors, then the threshold option is the 0 option, and we don't need to remove the Intercept created with modelling
cbc.mm = cbc.mm[, -4] # 1st item column is the 4th column (after resp.id, ques and alt)
items.columns = items.columns[-1]
}
#
#
#
# nalts = designctx$nalternatives
# mm = domodelmatrix(cbc.df$alt, nalts, getdfcolumns(cbc.df, cnames = names(designctx$covariates)))
# items.columns = attr(mm, "items.columns")
# cov.columns = attr(mm, "cov.columns")
#
# # remove the Intercept column - it is essentially contained in the alt column (Intercept is when all predictors are 0 - that leaves only the alt as the predictor)
# mm = mm[, -1]
# items.columns = items.columns[-1]
#
# cbc.mm = data.frame(cbind(resp.id = bw_encode_data_1(cbc.df$resp.id, nalts),
#                           ques = rep(rep(1:(2*designctx$nquestions), each = nalts), max(cbc.df$resp.id)),
# #                          alt = cbc.df$alt,
#                           alt = rep(1:nalts, nrow(mm)/nalts),
#                           mm,
#                           choice = bw_encode_data(cbc.df$best_choice, cbc.df$worst_choice, nalts)))
#
# helper za neke situacije, npr. za stan.mc
cbc.mm.clear = cbc.mm[, items.columns]
head(cbc.mm)
