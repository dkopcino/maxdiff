---
title:    |
    | MaxDiff TEST analysis
    | (an example)
author: "Danijel Kopčinović, IT Market"
#date: '15 10 2019 '
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics("upitnik_1.jpg")
```

## Introduction

In this document we will present a test market research with the MaxDiff method. The research is based on the (assumed) wish of a local pizzeria to find out which new pizzas would its buyers prefer to see in its offering.

New pizzas that the respondents choose from are:


```{r echo= FALSE, warning = FALSE, message = FALSE}
source("surveyconfig.R")

designctx = readRDS(designctxfile)
cbc.df = read.csv(answersfile, header = TRUE, sep = ",", quote = "\"", stringsAsFactors = FALSE)
simcoefs = NULL
if (file.exists(simulatecoefsfile)) simcoefs = readRDS(simulatecoefsfile)
r = lapply(colnames(designctx$survey), function(cn) {
  if (!is.null(cbc.df[[cn]])) {
    cbc.df[[cn]] <<- factor(cbc.df[[cn]], levels = levels(designctx$survey[[cn]]))
  }
})
r = lapply(colnames(designctx$fullfact_covdesign), function(cn) {
  if (!is.null(cbc.df[[cn]])) {
    cbc.df[[cn]] <<- factor(cbc.df[[cn]], levels = levels(designctx$fullfact_covdesign[[cn]]))
    # ovo dodajemo da izbjegnemo kombinaciju (0, 0) koja radi problem recimo za LCA: softmax(0, 0)
    # uvijek daje jednake vjerojatnosti za sve segmente pa ne možemo dobiti ništa osim toga
    contrasts(cbc.df[[cn]]) <<- contr.sum
  }
})
#if (!is.null(cbc.df$questionnaire.id)) cbc.df$questionnaire.id = NULL # maknemo questionnaire.id jer nam ne treba

print(designctx$items)

```


Except the main products (pizzas) that are in our focus, we will also take into account a possible influence of the "demographic" factors to the answers/results:


```{r echo= FALSE, warning = FALSE, message = FALSE}

print(designctx$covariates)

```

In a MaxDiff survey respondents choose the best and the worst product among a few alternatives. Let's have a look at some products combinations that we posed as questions.

```{r echo= FALSE, warning = FALSE, message = FALSE}

library(dplyr)
data.frame(designctx$survey[1:(5*designctx$nalternatives), ] %>% dplyr::group_by(task) %>% dplyr::mutate(items = paste0(alt, collapse = ","), best_choice = "?", worst_choice = "?") %>% dplyr::select("items", "best_choice", "worst_choice") %>% distinct())

```

Out of the set of all `r choose(length(designctx$items), designctx$nalternatives)` products combinations with `r designctx$nalternatives` alternatives (number of alternatives in a question is set as a design parameter), we have chosen  `r nrow(designctx$survey)` representative ones. With the chosen combinations it is possible to calculate (average or individual) value of each product for the respondents.

All combinations were divided to `r designctx$nquestionnaires` questionnaires, each with `r designctx$nquestions` questions, each with `r designctx$nalternatives` options/alternatives.

```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics("upitnik_2.jpg")
```

For a better estimation of the values of different products, we added a question in which a respondent expresses general positive or negative tendency to a few given products. The products given in this question are chosen related to the previous respondent's choices, so that a span, from the pizzas most often chosen as best to the pizzas most often chosen as worst, is given.
In this question products are not compared with each other but all against the so called threshold option, which can mean different things in different contexts, but generally means the point between want/don't want, would buy/wouldn't buy, am interested/am not interested etc. This question helps us to better level the answers of all the respondents against the threshold option and thus against each other, giving us better and more relevant product values estimates.


```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics("anchoring_survey.jpg")
```

Survey results were computer generated, with certain random parameters, for `r max(cbc.df$resp.id)` respondents.


## Simple analysis - counting

There are multiple ways to analyze the data with a MaxDiff method. The simplest approach is to count how many times some product was chosen as the best and as the worst. Here is the list of the products from our survey with the number of times chosen as best/worst:

```{r echo= FALSE, warning = FALSE, message = FALSE, fig.height = 4.3, fig.width = 10, fig.align = "center"}
library(dplyr)
library(DT)

bw.df = cbc.df %>% group_by(alt) %>% summarise(chosen_as_best = sum(best_choice), chosen_as_worst = sum(worst_choice))
# datatable(bw.df, rownames = FALSE, options = list(paging = FALSE, dom = 't')) %>%
#   formatStyle(columns = "chosen_as_best",
#               background = styleEqual(max(bw.df$chosen_as_best), "lightblue")) %>%
#   formatStyle(columns = "chosen_as_worst",
#               background = styleEqual(max(bw.df$chosen_as_worst), "pink"))

```


```{r echo= FALSE, warning = FALSE, message = FALSE, results = 'asis'}
library(knitr)
library(kableExtra)

# bw.df %>% mutate(chosen_as_best = cell_spec(chosen_as_best, "latex", color = ifelse(chosen_as_best == max(bw.df$chosen_as_best), "white", "black"), background = ifelse(chosen_as_best == max(bw.df$chosen_as_best), "blue", "white")),
#                  chosen_as_worst = cell_spec(chosen_as_worst, "latex", color = ifelse(chosen_as_worst == max(bw.df$chosen_as_worst), "white", "black"), background = ifelse(chosen_as_worst == max(bw.df$chosen_as_worst), "red", "white"))) %>%
# kable("latex", booktabs = T, linesep = "", align = "c")

kable(bw.df, "latex", booktabs = T, linesep = "", align = "c") %>%
  row_spec(which.max(bw.df$chosen_as_best), bold = T, color = "white", background = "blue") %>%
  row_spec(which.max(bw.df$chosen_as_worst), bold = T, color = "white", background = "red")


```

We can see that `r as.character(bw.df[which.max(bw.df$chosen_as_best), ]$alt)` was chosen most times as the best option, and `r as.character(bw.df[which.max(bw.df$chosen_as_worst), ]$alt)` most times as the worst option.


## Multinomial logit model analysis

More advanced analysis includes data (respondents' choices) modelling and then calculating the coefficients (values) for each product according to the multinomial logit model. After applying this analysis to our data, we get the following summary:


```{r echo= FALSE, warning = FALSE, message = FALSE}

bw_encode_vectors = function(v1, v2, nalts) {
  if (length(v1) != length(v2)) stop("invalid vectors' lengths")
  if ((length(v1) %% nalts) != 0) stop("invalid vectors' lengths vs. number of alternatives")
  v_coded = rep(0, 2*length(v1))
  for (q in 1:(length(v1)/nalts)) {
    i_start = (q-1)*nalts+1
    i_end = q*nalts
    v_coded[(q-1)*nalts+(i_start:i_end)] = v1[i_start:i_end]
    v_coded[q*nalts+(i_start:i_end)] = v2[i_start:i_end]
  }
  v_coded
}

bw_encode_dfs = function(v1, v2, nalts) {
  if ((nrow(v1) != nrow(v2)) || (ncol(v1) != ncol(v2))) stop("invalid data frames' dimensions")
  if ((nrow(v1) %% nalts) != 0) stop("invalid data frames' dimensions vs. number of alternatives")
  v_coded = getdfcolumns(v1[rep(1:nrow(v1), 2), ], cnames = colnames(v1))
  for (q in 1:(nrow(v1)/nalts)) {
    i_start = (q-1)*nalts+1
    i_end = q*nalts
    v_coded[(q-1)*nalts+(i_start:i_end), ] = v1[i_start:i_end, ]
    v_coded[q*nalts+(i_start:i_end), ] = v2[i_start:i_end, ]
  }
  v_coded
}

bw_encode_data = function(v1, v2, nalts) {
  if (is.vector(v1) && is.vector(v2)) {
    bw_encode_vectors(v1, v2, nalts)
  } else if (is.data.frame(v1) && is.data.frame(v2)) {
    bw_encode_dfs(v1, v2, nalts)
  } else if (is.matrix(v1) && is.matrix(v2)) {
    bw_encode_dfs(v1, v2, nalts)
  } else {
    stop("invalid input to bw_encode_data")
  }
}

bw_encode_data_1 = function(v1, nalts) {
  bw_encode_data(v1, v1, nalts)
}

bw_decode_vectors = function(v_coded, nalts) {
  if ((length(v_coded) %% (2*nalts)) != 0) stop("invalid vector length vs. number of alternatives")
  v = array(0, dim = c(length(v_coded)/2, 2))
  for (q in 1:(length(v_coded)/(2*nalts))) {
    i_start = (q-1)*(2*nalts)+1
    i_end = q*(2*nalts)
    j_start = i_start-(q-1)*nalts
    j_end = (i_start+nalts-1)-(q-1)*nalts
    v[j_start:j_end, 1] = v_coded[i_start:(i_start+nalts-1)]
    v[j_start:j_end, 2] = v_coded[(i_start+nalts):i_end]
  }
  v
}

bw_decode_dfs = function(v_coded, nalts) {
  if ((nrow(v_coded) %% (2*nalts)) != 0) stop("invalid data frame dimensions vs. number of alternatives")
  v = array(0, dim = c(length(v_coded)/2, 2))
  v1 = v2 = getdfcolumns(v_coded[1:(nrow(v_coded)/2), ], cnames = colnames(v_coded))
  for (q in 1:(nrow(v_coded)/(2*nalts))) {
    i_start = (q-1)*(2*nalts)+1
    i_end = q*(2*nalts)
    j_start = i_start-(q-1)*nalts
    j_end = (i_start+nalts-1)-(q-1)*nalts
    v1[j_start:j_end, ] = v_coded[i_start:(i_start+nalts-1), ]
    v2[j_start:j_end, ] = v_coded[(i_start+nalts):i_end, ]
  }
  list(v1, v2)
}

bw_decode_data = function(v_coded, nalts) {
  if (is.vector(v_coded)) {
    bw_decode_vectors(v_coded, nalts)
  } else if (is.data.frame(v_coded)) {
    bw_decode_dfs(v_coded, nalts)
  } else if (is.matrix(v_coded)) {
    bw_decode_dfs(v_coded, nalts)
  } else {
    stop("invalid input to bw_decode_data")
  }
}


# test.df = cbc.df[1:9, ]
# 
# bw_enc = bw_encode_data(test.df$best_choice, test.df$worst_choice, designctx$nalternatives)
# bw_dec = bw_decode_data(bw_enc, designctx$nalternatives)
# any(bw_dec[, 1] != test.df$best_choice)
# any(bw_dec[, 2] != test.df$worst_choice)
# 
# bw_enc = bw_encode_data(test.df, test.df, designctx$nalternatives)
# bw_dec = bw_decode_data(bw_enc, designctx$nalternatives)
# any(bw_dec[[1]] != test.df)
# any(bw_dec[[2]] != test.df)

# domodelmatrix = function(alt, nalts, covs = NULL, bwencode = TRUE) {
#   ret_matrix = encode_items(alt) # best choice encoded
#   items.columns = colnames(ret_matrix)
#   if (bwencode) ret_matrix = bw_encode_data(ret_matrix, (-1)*ret_matrix, nalts)
#   cov.columns = c()
#   if (!is.null(covs)) {
#     covs_encoded = encode_covariates(covs)
#     cov.columns = colnames(covs_encoded)
#     if (bwencode) covs_encoded = bw_encode_data(covs_encoded, (-1)*covs_encoded, nalts)
#     ret_matrix = cbind(ret_matrix, covs_encoded)
#   }
#   attr(ret_matrix, "items.columns") = items.columns
#   attr(ret_matrix, "cov.columns") = cov.columns
#   ret_matrix
# }

domodelmatrix_itemsonly = function(alt) {
  ret_matrix = encode_items(alt) # best choice encoded
  items.columns = colnames(ret_matrix)
  attr(ret_matrix, "items.columns") = items.columns
  ret_matrix
}

domodelmatrix_itemscovs = function(alt, covs) {
  ret_matrix = domodelmatrix_itemsonly(alt)
  items.columns = attr(ret_matrix, "items.columns")
  cov.columns = c()
  covs_encoded = encode_covariates(covs)
  cov.columns = colnames(covs_encoded)
  ret_matrix = cbind(ret_matrix, covs_encoded)
  attr(ret_matrix, "items.columns") = items.columns
  attr(ret_matrix, "cov.columns") = cov.columns
  ret_matrix
}

domodelmatrix_itemsonly_bwencode = function(alt, nalts) {
  ret_matrix = domodelmatrix_itemsonly(alt)
  items.columns = attr(ret_matrix, "items.columns")
  ret_matrix = bw_encode_data(ret_matrix, (-1)*ret_matrix, nalts)
  attr(ret_matrix, "items.columns") = items.columns
  ret_matrix
}

domodelmatrix_itemscovs_bwencode = function(alt, covs, nalts) {
  ret_matrix = domodelmatrix_itemscovs(alt, covs)
  items.columns = attr(ret_matrix, "items.columns")
  cov.columns =   attr(ret_matrix, "cov.columns")
  ret_matrix = bw_encode_data(ret_matrix, (-1)*ret_matrix, nalts)
  attr(ret_matrix, "items.columns") = items.columns
  attr(ret_matrix, "cov.columns") = cov.columns
  ret_matrix
}

# petlja za svaki resp.id
# enkodirati pitanja, covariates, best+worst choice
# ako ima, enkodirati anchor pitanja kao choice, dodati na kraj data.framea za resp.id
cbc.mm = data.frame(NULL)
for (resp.id in 1:max(cbc.df$resp.id)) {
  # izvuci subframe
  cbc.df.sf = cbc.df[cbc.df$resp.id == resp.id, ]
  nalts = designctx$nalternatives
  covs = getdfcolumns(cbc.df.sf, cnames = names(designctx$covariates))
  sf.mm = domodelmatrix_itemscovs_bwencode(cbc.df.sf$alt, covs, nalts)
  items.columns = attr(sf.mm, "items.columns")
  cov.columns = attr(sf.mm, "cov.columns")
  cbc.mm.sf = data.frame(cbind(resp.id = bw_encode_data_1(cbc.df.sf$resp.id, nalts), 
                               ques = rep(1:(2*max(cbc.df.sf$ques)), each = nalts), 
                               #alt = cbc.df$alt, 
                               alt = rep(1:nalts, nrow(sf.mm)/nalts),
                               sf.mm, 
                               choice = bw_encode_data(cbc.df.sf$best_choice, cbc.df.sf$worst_choice, nalts)))
  if (designctx$anchors > 0) {
    anchorcolnames = getanchorcolnames(designctx$anchors)
    anchorcols = cbc.df.sf[, colnames(cbc.df.sf) %in% anchorcolnames]
    best_choices = cbc.df.sf[cbc.df$best_choice == 1, ]$alt
    worst_choices = cbc.df.sf[cbc.df$worst_choice == 1, ]$alt
    anchors = getanchors(designctx$anchors, best_choices, worst_choices)
    anch.mm = domodelmatrix_itemscovs(anchors, getdfcolumns(covs[rep(1, length(anchors)), ], cnames = colnames(covs)))
    anch.mm = anch.mm[rep(1:nrow(anch.mm), each = 2), ]
    anch.mm[2*(1:(nrow(anch.mm)/2)), items.columns] = 0
    anch.choices = rep(t(anchorcols[1, ]), each = 2)
    anch.choices[2*(1:(length(anch.choices)/2))-1][anchorcols[1, ] == 2] = 0 # neparni
    anch.choices[2*(1:(length(anch.choices)/2))][anchorcols[1, ] == 1] = 0 # parni
    anch.choices[2*(1:(length(anch.choices)/2))][anchorcols[1, ] == 2] = 1 # parni
    anch.mm.sf = data.frame(cbind(resp.id = rep(resp.id, nrow(anch.mm)), 
                                  ques = rep(max(cbc.mm.sf$ques+1):(max(cbc.mm.sf$ques)+length(anchors)), each = 2), 
                                  alt = rep(1:2, nrow(anch.mm)/2),
                                  anch.mm,
                                  choice = anch.choices))
    cbc.mm.sf = rbind(cbc.mm.sf, anch.mm.sf)
  }

  cbc.mm = rbind(cbc.mm, cbc.mm.sf)
}

rownames(cbc.mm) = c()

if (designctx$anchors == 0) {
  # remove the Intercept column - it is essentially contained in the alt column (Intercept is when all predictors are 0 - that leaves only the alt as the predictor)
  # when we use anchors, then the threshold option is the 0 option, and we don't need to remove the Intercept created with modelling
  cbc.mm = cbc.mm[, -4] # 1st item column is the 4th column (after resp.id, ques and alt)
  items.columns = items.columns[-1]
}

# 
# 
# 
# nalts = designctx$nalternatives
# mm = domodelmatrix(cbc.df$alt, nalts, getdfcolumns(cbc.df, cnames = names(designctx$covariates)))
# items.columns = attr(mm, "items.columns")
# cov.columns = attr(mm, "cov.columns")
# 
# # remove the Intercept column - it is essentially contained in the alt column (Intercept is when all predictors are 0 - that leaves only the alt as the predictor)
# mm = mm[, -1]
# items.columns = items.columns[-1]
# 
# cbc.mm = data.frame(cbind(resp.id = bw_encode_data_1(cbc.df$resp.id, nalts), 
#                           ques = rep(rep(1:(2*designctx$nquestions), each = nalts), max(cbc.df$resp.id)), 
# #                          alt = cbc.df$alt, 
#                           alt = rep(1:nalts, nrow(mm)/nalts),
#                           mm, 
#                           choice = bw_encode_data(cbc.df$best_choice, cbc.df$worst_choice, nalts)))
# 
# helper za neke situacije, npr. za stan.mc
cbc.mm.clear = cbc.mm[, items.columns]

```

```{r echo= FALSE, warning = FALSE, message = FALSE}
#############################################################################################################
### >>> MIXED MNL
library(mlogit)

cov.df = data.frame() # ovo spremamo da bismo kasnije mogli raditi predikciju s istim ovim ispitanicima
# here we build fml1, which will be used to create a model and model matrix for mlogit
if (length(designctx$covariates) == 0) {
  fmlstring = paste("choice ~ 0 + ", paste(colnames(mm), collapse = "+")) # all vars have generic coefs
  fml1 = as.formula(fmlstring)
} else {
#  ncov.columns = setdiff(colnames(mm), cov.columns)
  ncov.columns = items.columns
  fmlstring1 = paste("0 +", paste(ncov.columns, collapse = "+")) # all but covariates have generic coefs
  fmlstring2 = paste("0 +", paste(cov.columns, collapse = "+")) # covariates will have alternative specific coefs
  fmlstring = paste("choice ~", fmlstring1, "|", fmlstring2)
  fml1 = mFormula(as.formula(fmlstring)) # ovdje je bitno da bude mFormula tako da se kod model.matrix pozove odgovarajuća funkcija!!! >> TO JE SAMO ZA mlogit, NE ZA choicemodelr!
  
  # fmlstring = paste("choice ~ 0 + ", paste(colnames(mm), collapse = "+")) # all vars have generic coefs
  # fml1 = mFormula(as.formula(fmlstring))
  # fml1 = as.formula(fmlstring)
  
  repeats = designctx$nquestions * designctx$nalternatives
  respondents = max(cbc.df$resp.id)
  cov.df = as.data.frame(cbc.df[(0:(respondents-1))*repeats+1, colnames(cbc.df) %in% names(designctx$covariates)])
  colnames(cov.df) = names(designctx$covariates)
}

# fml1 = as.formula(paste("choice ~ ", paste(g.columns, collapse = "+"), "| 0 |", paste(ascolumns, collapse = "+")))
# head(model.matrix(fml1, cbc.mlogit), 10)

vars_i = 4:(ncol(cbc.mm)-1) # skip over the first three: resp.id, ques, alt
cbc.mm$resp.ques = paste(cbc.mm$resp.id, ".", cbc.mm$ques, sep = "")
cbc.mlogit = mlogit.data(data = cbc.mm, choice = "choice", shape = "long", 
                         #varying = vars_i, 
                         alt.var = "alt", 
                         #alt.levels = levels(factor(cbc.mm$alt)), 
                         #alt.levels = unique(cbc.mm$alt), 
                         chid.var = "resp.ques",
                         id.var = "resp.id")

# ## BUILD AND SAVE MODEL
# # Ako imamo covariates, onda model prolazi samo ako se koeficijenti covariates računaju kao alternative specific
# # (znači u formuli idu iza | i s 0 +)
# m1 = mlogit(fml1, data = cbc.mlogit)
# # summary(m1)
# m1.rpar = rep("n", length = length(m1$coef)) # 'n' normal, 'l' log-normal, 't' truncated normal, 'u' uniform
# names(m1.rpar) = names(m1$coef)
# m2.hier = mlogit(fml1, data = cbc.mlogit, panel = TRUE, rpar = m1.rpar, correlation = TRUE)
# # summary(m1.hier)
# # m2.hier = update(m1.hier, correlation = TRUE)
# # summary(m2.hier)
# # SAVE/LOAD
# m2.hier$cov.df = cov.df
# saveRDS(m2.hier, mnlmodelfile)

## LOAD MODEL
if (!is.null(cbc.mm$resp.ques)) cbc.mm$resp.ques = NULL
m2.hier = readRDS(mnlmodelfile)

sm.mnl = summary(m2.hier)$summary.rpar
#print(sm.mnl[, !(colnames(sm.mnl) %in% c("Min.", "Median", "Max."))])

### MIXED MNL <<<
#############################################################################################################
```


```{r echo= FALSE, warning = FALSE, message = FALSE}
#############################################################################################################
### >>> HIERARCHICAL BAYES
library(ChoiceModelR)

# # modeliranje
cmr_choice = rep(0, nrow(cbc.mm))
cmr_choice[cbc.mm$alt == 1] = cbc.mm[cbc.mm$choice == 1, "alt"]
cmr.mm = cbind(cbc.mm[, colnames(cbc.mm) != "choice"], cmr_choice)
cmr.mm.bez.cov = cmr.mm[, !(colnames(cmr.mm) %in% cov.columns)]

demos = NULL
if (length(designctx$covariates) > 0) {
  # zbog ponavljanja demografije u redovima, trebamo izvaditi samo 1., 2., ... max(resp.id)
  max_resp_id = max(cmr.mm$resp.id)
  total_rows = nrow(cmr.mm)
  demos = as.matrix(cmr.mm[(0:(max_resp_id-1))*(total_rows/max_resp_id)+1, cov.columns])
}

# ## Build and save the model
# if (!is.null(demos)) {
#   hb.post = choicemodelr(data = cmr.mm.bez.cov,
#                          xcoding = rep(1, ncol(cmr.mm.bez.cov) - 4),
#                          demos = demos,
#                          mcmc = list(R = 20000, use = 10000),
#                          options = list(save = TRUE))
# } else {
#   hb.post = choicemodelr(data = cmr.mm.bez.cov,
#                          xcoding = rep(1, ncol(cmr.mm.bez.cov) - 4),
#                          mcmc = list(R = 20000, use = 10000),
#                          options = list(save = TRUE))
# }
# saveRDS(hb.post, hbmodelfile)
# ##

hb.post = readRDS(hbmodelfile)

# names(hb.post)
# betadraw su sva izvlačenja: ni (useri) x natt (atributi) x ndraws (izvlačenja)
# compdraw su sva izvlačenja srednjih vrijednosti atributa i (ko)varijanci: ndraw x (mu: natt, rooti: natt x natt) [bez utjecaja covariates, tj. kad su covariates = 0]
# deltadraw su sva izvlačenja adjustmentsa za covariates: ndraw x (ncovariates x natt)
# adjustments idu ovim redom: 1.cov/1.att, 2.cov/1.att, ..., n.cov/1.att, 1.cov/2.att, 2.cov/2.att...
# itd. sve do 1.cov/n.att, 2.cov/n.att, ... , n.cov/n.att
# u apply MARGIN označava indekse koje fiksiramo, a onda sve ostale indekse zbrojimo/izračunamo prosjek
# npr. ako je a 3x4x5 apply(a, 2:3, mean) će za svaki par iz svih parova 2.-og i 3.-eg indeksa sumirati sve elemente 
# za 1. indeks i izračunati sredinu; ukupno će rezultat biti 4x5
# za fiksiranog usera i atribut, izračunaj prosječnu vrijednost izvlačenja, odn. koeficijenta
# delta.means = apply(delta.d, c(1, 3), sum) # draws of total effect from all covariates on attributes

beta.post.mean = apply(hb.post$betadraw, 1:2, mean) # izračun svih prosječnih koeficijenata za svakog ispitanika
sm_means = apply(t(beta.post.mean), 1, mean)
beta.post.q25 = apply(hb.post$betadraw, 1:2, quantile, probs = c(0.25)) # isto kao gore, samo 25% kvantil
beta.post.q75 = apply(hb.post$betadraw, 1:2, quantile, probs = c(0.75)) # isto kao gore, samo 75% kvantil
sm_q25 = apply(t(beta.post.q25), 1, mean)
sm_q75 = apply(t(beta.post.q75), 1, mean)

sm.hb = cbind("1st Qu." = sm_q25, "Mean" = sm_means, "3rd Qu." = sm_q75)
rownames(sm.hb) = setdiff(colnames(cmr.mm.bez.cov), c("resp.id", "ques", "alt", "cmr_choice"))

sm.hb

### HIERARCHICAL BAYES <<<
#############################################################################################################

```



```{r echo= FALSE, warning = FALSE, message = FALSE}
#############################################################################################################
### >>> STAN MC PREPARATION
library(rstan)
#library(MASS)
#library(shinystan)

# TODO: Da li koristiti find_prior_for_beta? Ako da, onda izvrtiti skriptu i staviti dobiveni prior u STAN file.

# writes a compiled Stan program to the disk to avoid recompiling
rstan_options(auto_write = TRUE)

R = length(unique(cbc.mm$resp.id))
S = length(unique(cbc.mm$ques))
C = max(cbc.mm$alt)
K = ncol(cbc.mm.clear)
Y = matrix(cmr_choice[cmr_choice != 0], nrow = R, ncol = S, byrow = TRUE)
X = array(0, dim = c(R, S, C, K))
XC = array(C, dim = c(R, S)) # actual number of alternatives for each respondent/scenario
for (r in 1:R) { # respondents
  for (s in 1:S){ # choice scenarios - questions
    dm = data.matrix(cbc.mm.clear[(cbc.mm$resp.id == r) & (cbc.mm$ques == s), ])
    XC[r, s] = nrow(dm)
    if (nrow(dm) < C) dm = rbind(dm, matrix(0, nrow = (C - nrow(dm)), ncol = ncol(dm))) # add zero rows just to fill the X matrix
    X[r, s, , ] = dm
  }
}
if (!is.null(demos)) {
#  Z = t(demos)
  Z = array(t(demos[1:R, ]), dim = c(ncol(demos), R))
} else {
  Z = array(1, dim = c(1, R)) # intercept only
}
G = nrow(Z)

standata = list(C = C, K = K, R = R, S = S, G = G, Y = Y, X = X, XC = XC, Z = Z)
#str(standata)
rm(R, S, C, K, Y, X, XC, Z, G, r, s, c)

# Convergence checking gets tedious with a lot of parameters, so automate
stanmc_check_fit = function(fit) {
#  summ = summary(fit)$summary
  summ = na.omit(summary(fit)$summary) # if some is NA, all checks turn out bad - remove them
  range_rhat = range(summ[, 'Rhat'])
  rhat_ok = ((0.99 <= range_rhat[1]) && (range_rhat[2] <= 1.1))
  range_neff = range(summ[, 'n_eff'])
  neff_ok = (range_neff[1] >= 400)
  sp = rstan::get_sampler_params(fit, inc_warmup = FALSE)
  max_divergent = max(sapply(sp, function(p) { sum(p[, 'divergent__']) }))
  no_divergent = (max_divergent == 0)

  list(ok = rhat_ok && neff_ok && no_divergent,
       range_rhat = range_rhat,
       range_neff = range_neff,
       max_divergent = max_divergent)
}

### STAN MC PREPARATION <<<
```

```{r echo= FALSE, warning = FALSE, message = FALSE}
#############################################################################################################
### >>> STAN MC HIERARCHICAL BAYES
# ## BUILD THE MODEL
# cores = parallel::detectCores() - 1
# stan.mc = stan(file = "hmnl.stan",
#                data = standata,
#                verbose = FALSE,
#                chains = cores,
#                iter = 1000,
#                #algorithm = "HMC",
#                control = list(adapt_delta = 0.90, stepsize = 0.5),
#                cores = cores)
# 
# # plot(stan.mc, plotfun = "trace", pars = c("Theta"))
# # plot(stan.mc, plotfun = "trace", pars = c("tau"))
# # plot(stan.mc, plotfun = "trace", pars = c("Omega[1,2]"))
# # plot(stan.mc, plotfun = "trace", pars = paste("Beta[", 1:9, ", 1]", sep = "")) # resp 1
# # summary(stan.mc)$summary[, c("Rhat", "n_eff")]
# 
# # stanmc_check_fit(stan.mc) # should say ok = TRUE
# 
# saveRDS(stan.mc, stanmcmodelfile)
##

stan.mc = readRDS(stanmcmodelfile)

# sample ukupnih koeficijenata za atribute
stan.mc.betas = extract(stan.mc, pars = c("Beta"))$Beta
# sample srednjih vrijednosti koeficijenata beta modificiranih vrijednostima covariatesa
stan.mc.theta = apply(extract(stan.mc, pars = c("Theta"))$Theta, 2:3, mean)
colnames(stan.mc.theta) = cov.columns
stan.mc.omega = apply(extract(stan.mc, pars = c("Omega"))$Omega, 2:3, mean)
stan.mc.tau = apply(extract(stan.mc, pars = c("tau"))$tau, 2, mean)
# sample kovarijacijske matrice od beta
stan.mc.sigma = apply(extract(stan.mc, pars = c("Sigma"))$Sigma, 2:3, mean) # kovarijacijska matrica od beta
rownames(stan.mc.sigma) = colnames(stan.mc.sigma) = colnames(cbc.mm.clear)

# u apply MARGIN označava indekse koje fiksiramo, a onda sve ostale indekse zbrojimo/izračunamo prosjek
# npr. ako je a 3x4x5 apply(a, 2:3, mean) će za svaki par iz svih parova 2.-og i 3.-eg indeksa sumirati sve elemente 
# za 1. indeks i izračunati sredinu; ukupno će rezultat biti 4x5
# za fiksiranog usera i atribut, izračunaj prosječnu vrijednost izvlačenja, odn. koeficijenta
stan.mc.betas.mean = apply(stan.mc.betas, 2:3, mean) # izračun svih prosječnih koeficijenata za svakog ispitanika
smc_means = apply(stan.mc.betas.mean, 1, mean)
stan.mc.betas.q25 = apply(stan.mc.betas, 2:3, quantile, probs = c(0.25)) # isto kao gore, samo 25% kvantil
stan.mc.betas.q75 = apply(stan.mc.betas, 2:3, quantile, probs = c(0.75)) # isto kao gore, samo 75% kvantil
smc_q25 = apply(stan.mc.betas.q25, 1, mean)
smc_q75 = apply(stan.mc.betas.q75, 1, mean)

stan.mc.est = cbind("1st Qu." = smc_q25, "Mean" = smc_means, "3rd Qu." = smc_q75)
rownames(stan.mc.est) = setdiff(colnames(cmr.mm.bez.cov), c("resp.id", "ques", "alt", "cmr_choice"))

#stan.mc.est

### STAN MC HIERARCHICAL BAYES <<<
#############################################################################################################
```


```{r echo= FALSE, warning = FALSE, message = FALSE}
#############################################################################################################
### >>> STAN MC LCA
# sve ostalo je definirano gore kod stan.mc
standata$CL = designctx$rsegments

# BUILD MODEL
# cores = parallel::detectCores() - 1
# #cores = parallel::detectCores()
# # https://mc-stan.org/docs/2_20/reference-manual/hmc-algorithm-parameters.html
# stan.mc.lca = stan(file = "hmnl_lca.stan",
#                    data = standata,
#                    verbose = FALSE,
#                    chains = cores, # * 2,
# #                   iter = floor(2000 / 1),
#                    iter = 1000,
#                    #algorithm = "HMC",
#                    control = list(adapt_delta = 0.90, stepsize = 0.5), # def adapt_delta 0.8, stespize = 2.0
#                    cores = cores)
# # plot(stan.mc.lca, plotfun = "trace", pars = c("Theta"))
# # plot(stan.mc.lca, plotfun = "trace", pars = c("tau"))
# # plot(stan.mc.lca, plotfun = "trace", pars = c("Omega[1,2]"))
# # plot(stan.mc.lca, plotfun = "trace", pars = paste("Beta[", 1:9, ", 1]", sep = "")) # resp 1
# # summary(stan.mc.lca)$summary[, c("Rhat", "n_eff")]
# # stanmc_check_fit(stan.mc.lca) # should say ok = TRUE
# 
# saveRDS(stan.mc.lca, stanmclcamodelfile)
##

stan.mc.lca = readRDS(stanmclcamodelfile)


# ## dijagnostika
# print(stan.mc.lca)
# # https://betanalpha.github.io/assets/case_studies/rstan_workflow.html
# source('stan_utility.R')
# lsf.str()
# check_n_eff(stan.mc.lca)
# check_rhat(stan.mc.lca)
# check_treedepth(stan.mc.lca, 11) # fit <- stan(file='eight_schools_cp.stan', data=data, seed=194838, control=list(max_treedepth=15))
# check_energy(stan.mc.lca)
# check_div(stan.mc.lca) # fit <- stan(file='eight_schools_cp.stan', data=data, seed=194838, control=list(adapt_delta=0.90))
# 
# library(shinystan)
# launch_shinystan(stan.mc.lca)
# launch_shinystan(stan.mc)
# ##

# sample ukupnih koeficijenata za atribute
stan.mc.lca.betas = extract(stan.mc.lca, pars = c("Beta"))$Beta
# sample kovarijacijske matrice od beta
stan.mc.lca.betasigma = apply(extract(stan.mc.lca, pars = c("BetaSigma"))$BetaSigma, 2:3, mean)
rownames(stan.mc.lca.betasigma) = colnames(stan.mc.lca.betasigma) = colnames(cbc.mm.clear)

# sample ukupnih koeficijenata između segmenata i user covariatesa
stan.mc.lca.gamma = extract(stan.mc.lca, pars = c("Gamma"))$Gamma
# sample kovarijacijske matrice od gamma
stan.mc.lca.gammasigma = apply(extract(stan.mc.lca, pars = c("GammaSigma"))$GammaSigma, 2:3, mean) 

# sample vjerojatnosti pripadanja respondenta segmentu
stan.mc.lca.theta = extract(stan.mc.lca, pars = c("Theta"))$Theta

# sređivanje summarya za daljnje korištenje
stan.mc.lca.betas.mean = apply(stan.mc.lca.betas, 2:3, mean)
rownames(stan.mc.lca.betas.mean) = setdiff(colnames(cmr.mm.bez.cov), c("resp.id", "ques", "alt", "cmr_choice"))
colnames(stan.mc.lca.betas.mean) = paste("segment_", 1:ncol(stan.mc.lca.betas.mean), sep = "")
smclca_means = apply(stan.mc.lca.betas.mean, 1, mean)
stan.mc.lca.betas.q25 = apply(stan.mc.lca.betas, 2:3, quantile, probs = c(0.25))
stan.mc.lca.betas.q75 = apply(stan.mc.lca.betas, 2:3, quantile, probs = c(0.75))
smclca_q25 = apply(stan.mc.lca.betas.q25, 1, mean)
smclca_q75 = apply(stan.mc.lca.betas.q75, 1, mean)

stan.mc.lca.est = cbind("1st Qu." = smclca_q25, "Mean" = smclca_means, "3rd Qu." = smclca_q75)
rownames(stan.mc.lca.est) = rownames(stan.mc.lca.betas.mean) # isto kao gore

#stan.mc.lca.est

### STAN MC LCA <<<
#############################################################################################################
```


```{r echo= FALSE, warning = FALSE, message = FALSE}

# koji ćemo koristiti?
sm = sm.hb
# sm = stan.mc.est
# sm = stan.mc.lca.est


# TODO: dodati neku usporedbu, AIC, WAIC, log-likelihood ili sl. da nekako vidimo koliko su dobri modeli?


```

The given summary contains a lot of information which is not so easy to interpret directly. But to clarify a bit, here are a few explanations derived directly from the summary:

* 'calzone' has a negative (mean) coefficient `r as.character(round(sm['calzone', 'Mean'], 3))`, which means that the respondents value it less than the basic threshold option representing the would/would not buy point (coefficients are always calculated with respect to the first/basic declared option, which is in this case the threshold option) - to put it simply, respondents would not buy 'calzone', and due to a relatively big negative coefficient, this decision is pretty strong,

* 'pikantna' and 'ribarska' both have positive (mean) coefficients (`r as.character(round(sm['pikantna', 'Mean'], 3))`, `r as.character(round(sm['ribarska', 'Mean'], 3))`) which means that the respondent value both of them more than the basic threshold option, but 'pikantna' is valued more than 'ribarska' - to put it simply, respondents would buy both of them, but 'pikantna' rather than 'ribarska',

* the first and third quantiles define the span containing (this is somewhat simplified) each coefficient with the 75% probability.

If we normalize and scale all the coefficients so that the threshold option (would/would not buy) maps to the number 100, it is easier to interpret their values and corresponding ratios:

```{r echo= FALSE, warning = FALSE, message = FALSE}

library(ggplot2)
# zero-center the coefficents
zc.sm = (sm[, "Mean"] - mean(sm[, "Mean"]))
prob.zc.sm = exp(zc.sm)/(exp(zc.sm)+designctx$nalternatives-1)
perc.zc.sm = (prob.zc.sm*100)/(1/designctx$nalternatives)
#perc.zc.sm = ((prob.zc.sm/sum(prob.zc.sm))*100)
ggplot(data = data.frame(product = paste(names(perc.zc.sm), " (", round(perc.zc.sm, digits = 2), ")", sep = ""), value = perc.zc.sm), aes(x = product, y = value)) + geom_col(fill = "blue") + geom_hline(aes(yintercept = 100), linetype = "dashed", color = "red") +  labs(y = "ne bih kupio.......kupio bih", color = "red") + theme(axis.title.x = element_text(hjust = .23), axis.title.y = element_blank()) + coord_flip()

mostvpi = which.max(perc.zc.sm)
mostvpv = perc.zc.sm[mostvpi]
mostvpn = names(mostvpv)
compvpv = perc.zc.sm[mostvpi+1]
compvpn = names(compvpv)
rationmostcomp = round(as.numeric(mostvpv/compvpv), digits = 2)
moreless = if (rationmostcomp > 1) "more" else "less"

```

Now we can see that the most valued product is `r mostvpn` and that its value is `r rationmostcomp` times `r moreless` than the value of `r compvpn`.

These results show the main advantages of the MaxDiff analysis compared to a simple product "ordering" from the best to the worst which is sometimes (too often :)) used for business analyses. MaxDiff shows us:

* *relations between products* - how much is each product more or less valuable to the respondents,
* product positions *related to (key!) point marking the positive/negative attitude* - would or would not buy, am or am not interested...

This way we get a complete picture of all the products from the respondents' perspective and we can make quality business decisions.


## Products correlation analysis


```{r echo= FALSE, warning = FALSE, message = FALSE}
# cc = cov2cor(cov.mlogit(m2.hier))
cc = stan.mc.sigma
# cc = stan.mc.lca.betasigma
# cc

# locate maximal element
# i = (which(cc == max(cc[which(cc < 1)])))[1]
# dn1 = dimnames(cc)[[1]]
# d = length(dn1)
# n1 = dn1[ceiling(i/d)]
# n2 = dn1[i %% d]

# pick a random variable to check correlations
#corvar = sample(colnames(cbc.mm.clear), 1)
corvar = "ribarska"
# correlation = cc[which((abs(cc[ , corvar]) > 0.4) & (abs(cc[ , corvar]) < 1)), corvar]
pos_correlated = which((cc[, corvar] < 1) & (cc[ , corvar] > 0))
neg_correlated = which(cc[ , corvar] < 0)
if (length(pos_correlated) < 3) {
  correlation = cc[union(neg_correlated[1:min((5-length(pos_correlated)), length(neg_correlated))], pos_correlated), corvar]
} else if (length(neg_correlated) < 3) {
  correlation = cc[union(pos_correlated[1:min((5-length(neg_correlated)), length(pos_correlated))], neg_correlated), corvar]
} else {
  correlation = cc[union(neg_correlated[1:2], pos_correlated[1:3]), corvar]
}
# plot(correlation, main = paste("Correlations between", corvar, "and other variables"), xaxt = "n", 
#      cex = 3, col = "red", ylim = c(1.1*min(correlation), 1.1*max(correlation)))
# axis(side = 1, at = 1:4, labels = names(correlation))

correlation_t = as.data.frame(correlation)
correlation_t$names = row.names(correlation_t)

```

Another element that we can analyze is the correlation between the products/products values.


```{r echo= FALSE, warning = FALSE, message = FALSE}

library(ggplot2)
ggplot(data = correlation_t, aes(x = substr(correlation_t$names, 1, 18), y = correlation)) + geom_col(color = "red", fill = "pink", width = .03) +
  geom_hline(aes(yintercept = 0), color = "red") + xlab("") + ggtitle(paste("Correlations between", corvar, "and other values"))

```


For example, in the graph shown above, we can see that *`r corvar`* is positively correlated with *`r names(correlation[correlation > 0])`*, and negatively correlated with *`r names(correlation[correlation < 0])`*. This means that if respondents put more value to the *`r corvar`* pizza, then they put more value also to *`r names(correlation[correlation > 0])`*, while they put less value to *`r names(correlation[correlation < 0])`*.

Results of the analysis and coefficients estimation is the easiest to understand with the share prediction calculation, which we will now show.


## Share prediction

Once we have all the product coefficients estimates, we can compare different product combinations and predict the sales shares. Predicted sales shares are probabilities that a certain product will be chosen/sold compared to other products in the tested combination.

As an example we will pick a combination of a few randomly selected products and calculate their predicted sales shares:


```{r echo= FALSE, warning = FALSE, message = FALSE}

library(MASS)

# model must contain cov.df (can be empty)
# data must be a data.frame with the items alternatives in the 1st column
# if resp_i is not NULL, calculate utilities for only 1 respondent
# if cov.df is not empty, take the covariates from it for the respondent with index resp_i
# if resp_i is NULL, calculate average for either nrow(cov.df) respondents if cov.df is not empty or for 1000 respondents (without any covariates)
# returns matrix of utilities, nresp x nrow(data)
calc.utilities.mnl = function(model, data, resp_i = NULL) {
  
  # alt is the alternatives column, must be prepared to meet mlogit conditions
  # resp_cov is a row of respondent's covariates (must be a data.frame, can be empty)
  # returns matrix-row of utilities, 1 x nrow(data)
  calc.utilities.mnl_resp = function(model, data, alt, resp_cov) {
    coef.Sigma = cov.mlogit(model)
    coef.mu = model$coef[1:dim(coef.Sigma)[1]]
    draw_i = mvrnorm(1, coef.mu, coef.Sigma)
    nalts = designctx$nalternatives
  
    if (nrow(resp_cov) > 0) {
      # add covariates to the data frame
      # n_d = ncol(data)
      # data = cbind(data, resp_cov[rep(1, nrow(data)), ])
      # colnames(data)[(n_d+1):ncol(data)] = colnames(resp_cov)
      resp_cov = getdfcolumns(resp_cov[rep(1, nrow(data)), ], cnames = colnames(resp_cov))
      ffd_mm = domodelmatrix_itemscovs(data[, 1], resp_cov)
    } else {
      ffd_mm = domodelmatrix_itemsonly(data[, 1])
    }
  
    if (designctx$anchors == 0) ffd_mm = ffd_mm[, -1] # without the Intercept
    ffd_mm$alt = alt
    ffd_mm$choice = rep(1, nrow(ffd_mm))
    ffd.mlogit = mlogit.data(data = ffd_mm, choice = "choice", shape = "long", varying = 1:(ncol(ffd_mm)-2), alt.var = "alt", alt.levels = levels(ffd_mm$alt))
    data.model = model.matrix(model$formula, data = ffd.mlogit)
    utilities = data.model %*% draw_i
    t(utilities) # to get a row vector
  }
  
  # returns matrix of utilities, 1000 x nrow(data)
  calc.utilities.mnl_resps_without_cov = function(model, data, alt) {
    nresp = 1000
    utilities = matrix(NA, nrow = nresp, ncol = nrow(data))
    for (i in 1:nresp) {
      utilities[i, ] = calc.utilities.mnl_resp(model, data, alt, data.frame())
    }
    utilities
  }
  
  # returns matrix of utilities, nrow(resp_cov) x nrow(data)
  calc.utilities.mnl_resps_with_cov = function(model, data, alt, resp_cov) {
    nresp = nrow(resp_cov)
    utilities = matrix(NA, nrow = nresp, ncol = nrow(data))
    for (i in 1:nresp) {
      utilities[i, ] = calc.utilities.mnl_resp(model, data, alt, getdfcolumns(resp_cov[i, ], cnames = colnames(resp_cov)))
    }
    utilities
  }

  # ovo moramo napraviti zbog uvjeta u mlogit.data (broj redaka mora biti višekratnik od broja alternativa)
  # ako broj redaka nije višekratnik, onda ćemo (bez utjecaja na izračun utilitya) iskopirati na kraj data
  # još n_add redaka (1. redak), a kasnije ćemo to maknuti
  n_add = 0
  altlevels = as.numeric(levels(attributes(model$model$choice)$index$alt))
  # ovo moramo napraviti iz istog razloga kao gore, s time da alt moramo izvući iz modela
  # ne možemo staviti bezveze jer ako npr. koristimo covariates, onda su njihovi koeficijenti vezani
  # za alternativu (alternative specific) pa ovise o alternativi i alternativa mora biti definirana
  # kako je ovo samo za predikciju, postavljamo alt za sve podatke na istu vrijednost (nadamo se da će
  # relativni omjeri i dalje ostati isti)
  n_alts = length(altlevels)
  
  n_df = nrow(data)
  if ((n_df %% n_alts) > 0) {
    n_add = ceiling(n_df/n_alts)*n_alts - n_df
    data.add = getdfcolumns(data[rep(1, n_add), ], cnames = colnames(data))
    data = rbind(data, data.add)
  }
  alt = factor(rep(altlevels, nrow(data)/n_alts), levels = altlevels)
  
  if (is.null(resp_i)) {
    if (nrow(model$cov.df) > 0) {
      utilities = calc.utilities.mnl_resps_with_cov(model, data, alt, model$cov.df)
    } else {
      utilities = calc.utilities.mnl_resps_without_cov(model, data, alt)
    }
  } else {
    if (nrow(model$cov.df) > 0) {
      utilities = calc.utilities.mnl_resp(model, data, alt, getdfcolumns(model$cov.df[resp_i, ], cnames = colnames(model$cov.df)))
    } else {
      utilities = calc.utilities.mnl_resp(model, data, alt, data.frame())
    }
  }
  if (n_add > 0) utilities = utilities[, -((ncol(utilities)-n_add+1):ncol(utilities))]
  utilities

}

# Simulating shares
# model must contain cov.df (can be empty)
# data must be a data.frame with the items alternatives in the 1st column
# see also calc.utilities.mnl
predict.hier.mnl = function(model, data) {
  utilities = calc.utilities.mnl(model, data)
  nresp = nrow(utilities)
  shares = matrix(NA, nrow = nresp, ncol = ncol(utilities))
  for (i in 1:nresp) {
    utility = utilities[i, ]
    share = exp(utility)/sum(exp(utility))
    shares[i, ] = round(share * 100, digits = 2) # round to get percentage
  }
  r = cbind(data, "share %" = colMeans(shares))
  rownames(r) = c()
  r[["share %"]] = as.numeric(round(r[["share %"]], digits = 2))
  r
}

# Predict choice for data for respondent 
# model must contain cov.df (can be empty)
# data must be a data.frame with the items alternatives in the 1st column
# resp_i is the i/id of the respondent
# see also calc.utilities.mnl
predict.hier.mnl.choice = function(model, data, resp_i) {
  utilities = calc.utilities.mnl(model, data, resp_i)
  share = exp(utilities)/sum(exp(utilities))
  shares = round(share * 100, digits = 2) # round to get percentage
  best_choice = worst_choice = rep(0, nrow(data))
  best_choice[which.max(shares)] = 1
  worst_choice[which.min(shares)] = 1
  r = cbind(data, "share %" = shares, best_choice = best_choice, worst_choice = worst_choice)
  rownames(r) = c()
  r[["share %"]] = as.numeric(round(r[["share %"]], digits = 2))
  r
}

# data must be a data.frame with the items alternatives in the 1st column
# resp_i is a respondent's identifier in the model (can be NULL)
# returns matrix of utilities, nresp x nrow(data) x ndraws
calc.utilities.hb = function(model, data, resp_i = NULL) {
  data.df = data.frame(domodelmatrix_itemsonly(data[, 1]))
  if (designctx$anchors == 0) data.df = data.df[, -1] # without the Intercept
  data.model = as.matrix(data.df)
  betadraw = model$betadraw
  nresps = 1:(dim(betadraw)[1])
  if (!is.null(resp_i)) nresps = array(resp_i, dim = c(1))
  ndraws = dim(betadraw)[3]
  utilities = array(dim = c(length(nresps), nrow(data.df), ndraws))
  for (d in 1:ndraws) {
    for (i in 1:length(nresps)) {
      utilities[i, , d] = data.model %*% betadraw[nresps[i], , d]
    }
  }
  utilities
}

# data must be a data.frame with the items alternatives in the 1st column
# resp_i is a respondent's identifier in the model
# returns vector of utilities, nrow(data)
calc.utilities.hb.zc = function(model, data, resp_i) {
  data.df = data.frame(domodelmatrix_itemsonly(data[, 1]))
  if (designctx$anchors == 0) data.df = data.df[, -1] # without the Intercept
  data.model = as.matrix(data.df)
  betadraw = model$betadraw[resp_i, , ] # get coeffs x draws
  betas = apply(betadraw, 1, mean) # get mean of coeffs wrt draws
  as.numeric(data.model %*% (betas - mean(betas)))
}

### HIERARCHICAL BAYES
# data must be a data.frame
# resp_i is a respondent's identifier in the model (can be NULL) 
# see also calc.utilities.hb
predict.hb.mnl = function(model, data, resp_i = NULL) {
  utilities = calc.utilities.hb(model, data, resp_i)
  betadraw = model$betadraw
  nresps = 1:(dim(betadraw)[1])
  if (!is.null(resp_i)) nresps = array(resp_i, dim = c(1))
  ndraws = dim(betadraw)[3]
  shares = array(dim = c(length(nresps), nrow(data), ndraws))
  for (d in 1:ndraws) {
    for (i in 1:length(nresps)) {
      shares[i, , d] = (exp(utilities[i, , d])/sum(exp(utilities[i, , d]))) * 100
    }
  }
  shares.agg = apply(shares, 2:3, mean)
  r = cbind(data, 
            "share %" = apply(shares.agg, 1, mean)
            # ,
            # "5%" = apply(shares.agg, 1, quantile, probs = c(0.05)),
            # "95%" = apply(shares.agg, 1, quantile, probs = c(0.95))
            )
  rownames(r) = c()
  r[["share %"]] = as.numeric(round(r[["share %"]], digits = 2))
  # r[["5%"]] = as.numeric(round(r[["5%"]], digits = 2))
  # r[["95%"]] = as.numeric(round(r[["95%"]], digits = 2))
  r
}

### HIERARCHICAL BAYES
# data must be a data.frame
# resp_i is a respondent's identifier in the model
# see also predict.hb.mnl and calc.utilities.hb
predict.hb.mnl.choice = function(model, data, resp_i) {
  shares = predict.hb.mnl(model, data, resp_i)
  best_choice = worst_choice = rep(0, nrow(data))
  best_choice[which.max(shares[["share %"]])] = 1
  worst_choice[which.min(shares[["share %"]])] = 1
  r = cbind(data, "share %" = shares[["share %"]], best_choice = best_choice, worst_choice = worst_choice)
  rownames(r) = c()
  r
}

# data must be a data.frame
# resp_i is a respondent's identifier in the model (can be NULL)
# returns matrix of utilities, nresp x nrow(data) x ndraws
calc.utilities.stan.mc = function(model, data, resp_i = NULL) {
  data.df = data.frame(domodelmatrix_itemsonly(data[, 1]))
  if (designctx$anchors == 0) data.df = data.df[, -1] # without the Intercept
  data.model = as.matrix(data.df)
  betas = extract(model, pars = c("Beta"))$Beta
  nresps = 1:(dim(betas)[3])
  if (!is.null(resp_i)) nresps = array(resp_i, dim = c(1))
  ndraws = dim(betas)[1]
  utilities = array(dim = c(length(nresps), nrow(data.df), ndraws))
  for (d in 1:ndraws) {
    for (i in 1:length(nresps)) {
      utilities[i, , d] = data.model %*% betas[d, , nresps[i]] # turn i and d in betas
    }
  }
  utilities
}

### HIERARCHICAL BAYES
# data must be a data.frame
# resp_i is a respondent's identifier in the model (can be NULL)
# see also calc.utilities.stan.mc
predict.stan.mc = function(model, data, resp_i = NULL) {
  utilities = calc.utilities.stan.mc(model, data)
  betas = extract(model, pars = c("Beta"))$Beta
  nresps = 1:(dim(betas)[3])
  if (!is.null(resp_i)) nresps = array(resp_i, dim = c(1))
  ndraws = dim(betas)[1]
  shares = array(dim = c(length(nresps), nrow(data), ndraws))
  for (d in 1:ndraws) {
    for (i in 1:length(nresps)) {
      shares[i, , d] = (exp(utilities[i, , d])/sum(exp(utilities[i, , d]))) * 100
    }
  }
  shares.agg = apply(shares, 2:3, mean)
  r = cbind(data, 
            "share %" = apply(shares.agg, 1, mean)
            # ,
            # "5%" = apply(shares.agg, 1, quantile, probs = c(0.05)),
            # "95%" = apply(shares.agg, 1, quantile, probs = c(0.95))
            )
  rownames(r) = c()
  r[["share %"]] = as.numeric(round(r[["share %"]], digits = 2))
  # r[["5%"]] = as.numeric(round(r[["5%"]], digits = 2))
  # r[["95%"]] = as.numeric(round(r[["95%"]], digits = 2))
  r
}

### HIERARCHICAL BAYES
# data must be a data.frame
# resp_i is a respondent's identifier in the model
# see also predict.stan.mc and calc.utilities.stan.mc
predict.stan.mc.choice = function(model, data, resp_i) {
  shares = predict.stan.mc(model, data, resp_i)
  best_choice = worst_choice = rep(0, nrow(data))
  best_choice[which.max(shares[["share %"]])] = 1
  worst_choice[which.min(shares[["share %"]])] = 1
  r = cbind(data, "share %" = shares[["share %"]], best_choice = best_choice, worst_choice = worst_choice)
  rownames(r) = c()
  r
}

```


```{r echo= FALSE, warning = FALSE, message = FALSE}

# fullfact_design = combn(designctx$items, designctx$nalternatives)
# ffd_df = fullfact_design[, sample(1:ncol(fullfact_design), 10, replace = FALSE)]
# ffd_df = unlist(lapply(1:ncol(ffd_df), function(c) ffd_df[, c])) # data treba biti jednostavna lista opcija, ostalo će se kreirati
# ffd_df = data.frame(opcije = ffd_df)

# trebalo bi uspoređivati samo kombinacije s brojem opcija jednakim kao što je bilo kod kreiranja modela (nalternatives)
# ali prema ormeu svi to rade bez obzira na broj opcija pa ćemo i mi
#ffd_df = data.frame(opcije = designctx$items)
ffd_df = data.frame(opcije = designctx$items[sample(5)])

# fullfact_covdesign = designctx$fullfact_covdesign
# if (length(designctx$covariates) > 0) {
#   ffd_covdf = fullfact_covdesign[sample(1:nrow(fullfact_covdesign), 10, replace = TRUE), ]
#   ffd_df = cbind(ffd_df, ffd_covdf)
# }

shares.mnl = predict.hier.mnl(m2.hier, data = ffd_df)
shares.hb = predict.hb.mnl(hb.post, data = ffd_df)
shares.stan.mc = predict.stan.mc(stan.mc, data = ffd_df)
shares.stan.mc.lca = predict.stan.mc(stan.mc.lca, data = ffd_df)

shares = shares.hb
shares = shares.stan.mc
shares = shares.stan.mc.lca

#shares
mostsharesindex = which.max(shares[["share %"]])
mostshare = shares[mostsharesindex, ]

t(shares)

```


In the table above we can see how would respondents choose between `r nrow(ffd_df)` different pizzas. Each pizza gets the predicted share (percentage) in the total sales, according to the corresponding coefficients estimates.

**Most often chosen option** (among the given pizzas, not in general) is *`r paste(as.matrix(mostshare[, -ncol(mostshare)]), collapse = ", ")`* **with the predicted sales share `r max(shares[["share %"]])`%**.



## Checking the model on the existing data

With the share predictions for different products combinations, we can also test how would our model predict the choices for (some) of the combinations given in the survey:


```{r echo= FALSE, warning = FALSE, message = FALSE}

holdout_respondents_n = 30
holdout_respondents_q = 3
holdout_respondents = sample(1:max(cbc.df$resp.id), holdout_respondents_n, replace = FALSE)
holdout_questions = sample(1:max(cbc.df$ques), holdout_respondents_q, replace = FALSE)
keep_cols = c("alt")

rgrid = expand.grid(holdout_questions, holdout_respondents)
holdout_true_predicted = matrix(
  NA, 
  nrow = holdout_respondents_n * holdout_respondents_q, 
  ncol = 6, 
  dimnames = list(
    "respondent/question" = paste(rgrid$Var2, "/", rgrid$Var1, sep = ""),
    "choice" = c("true best", "predicted best", "correct best", "true worst", "predicted worst", "correct worst")
  )
)

holdout_true_predicted_mnl = holdout_true_predicted_hb = holdout_true_predicted_stan_mc = holdout_true_predicted_stan_mc_lca = holdout_true_predicted
for (i in 1:holdout_respondents_n) { # respondent
  for (j in 1:holdout_respondents_q) { # question
    ffd_df = cbc.df[(cbc.df$resp.id == holdout_respondents[i]) & (cbc.df$ques == holdout_questions[j]), ]
    true_best_choice = which(ffd_df$best_choice == 1)
    true_worst_choice = which(ffd_df$worst_choice == 1)
    ffd_df = getdfcolumns(ffd_df, cnames = keep_cols)
    
    # predicted = predict.hier.mnl.choice(m2.hier, ffd_df, holdout_respondents[i])
    # predicted_best_choice = which(predicted$best_choice == 1)
    # predicted_worst_choice = which(predicted$worst_choice == 1)
    # holdout_true_predicted_mnl[(i-1)*holdout_respondents_q+j, ] = 
    #   c(true_best_choice, predicted_best_choice, true_best_choice == predicted_best_choice,
    #     true_worst_choice, predicted_worst_choice, true_worst_choice == predicted_worst_choice)
    # 
    predicted = predict.hb.mnl.choice(hb.post, ffd_df, holdout_respondents[i])
    predicted_best_choice = which(predicted$best_choice == 1)
    predicted_worst_choice = which(predicted$worst_choice == 1)
    holdout_true_predicted_hb[(i-1)*holdout_respondents_q+j, ] = 
      c(true_best_choice, predicted_best_choice, true_best_choice == predicted_best_choice,
        true_worst_choice, predicted_worst_choice, true_worst_choice == predicted_worst_choice)

    # predicted = predict.stan.mc.choice(stan.mc, ffd_df, holdout_respondents[i])
    # predicted_best_choice = which(predicted$best_choice == 1)
    # predicted_worst_choice = which(predicted$worst_choice == 1)
    # holdout_true_predicted_stan_mc[(i-1)*holdout_respondents_q+j, ] =
    #   c(true_best_choice, predicted_best_choice, true_best_choice == predicted_best_choice,
    #     true_best_choice, predicted_worst_choice, true_worst_choice == predicted_worst_choice)
    # 
    # predicted = predict.stan.mc.choice(stan.mc.lca, ffd_df, holdout_respondents[i])
    # predicted_best_choice = which(predicted$best_choice == 1)
    # predicted_worst_choice = which(predicted$worst_choice == 1)
    # holdout_true_predicted_stan_mc_lca[(i-1)*holdout_respondents_q+j, ] =
    #   c(true_best_choice, predicted_best_choice, true_best_choice == predicted_best_choice,
    #     true_best_choice, predicted_worst_choice, true_worst_choice == predicted_worst_choice)
  }
}

# sum(holdout_true_predicted_mnl[, "correct best"])/nrow(holdout_true_predicted_mnl)
# sum(holdout_true_predicted_mnl[, "correct worst"])/nrow(holdout_true_predicted_mnl)
# sum(holdout_true_predicted_hb[, "correct best"])/nrow(holdout_true_predicted_hb)
# sum(holdout_true_predicted_hb[, "correct worst"])/nrow(holdout_true_predicted_hb)
# sum(holdout_true_predicted_stan_mc[, "correct best"])/nrow(holdout_true_predicted_stan_mc)
# sum(holdout_true_predicted_stan_mc[, "correct worst"])/nrow(holdout_true_predicted_stan_mc)
# sum(holdout_true_predicted_stan_mc_lca[, "correct best"])/nrow(holdout_true_predicted_stan_mc_lca)
# sum(holdout_true_predicted_stan_mc_lca[, "correct worst"])/nrow(holdout_true_predicted_stan_mc_lca)

#holdout_true_predicted = holdout_true_predicted_mnl
holdout_true_predicted = holdout_true_predicted_hb
#holdout_true_predicted = holdout_true_predicted_stan_mc
#holdout_true_predicted = holdout_true_predicted_stan_mc_lca

holdout_true_predicted[sample(1:nrow(holdout_true_predicted), 20), !(colnames(holdout_true_predicted) %in% c("correct best", "correct worst"))]
#holdout_true_predicted[sample(1:nrow(holdout_true_predicted), 20), ]
total_correct = sum(holdout_true_predicted[, "correct best"]) + sum(holdout_true_predicted[, "correct worst"])
paste("Overall correct predictions:", round(100*total_correct/(2*nrow(holdout_true_predicted)), digits = 2), "%", sep = "")

```

The overall prediction correctness gives us confidence that our model works well.


## Respondents segmentation

```{r echo= FALSE, warning = FALSE, message = FALSE}
#############################################################################################################
# ### >>> GMNL LCA: https://rpubs.com/msarrias1986/335556
# if (length(designctx$covariates) == 0) {
#   # fmlstring = paste("choice ~ 0 + ", paste(colnames(mm), collapse = "+")) # all vars have generic coefs
#   fmlstring = paste("choice ~ ", paste(colnames(mm), collapse = "+")) # all vars have generic coefs
#   fmlstring = paste(fmlstring, "| 0 | 0 | 0 | 1")
#   fml2 = as.formula(fmlstring)
# } else {
# #  ncov.columns = setdiff(colnames(mm), cov.columns)
#   ncov.columns = items.columns
#   # fmlstring1 = paste("0 +", paste(ncov.columns, collapse = "+")) # all but covariates have generic coefs
#   fmlstring1 = paste(ncov.columns, collapse = "+") # all but covariates have generic coefs
#   # fmlstring2 = paste("0 +", paste(cov.columns, collapse = "+"))
#   fmlstring2 = paste(cov.columns, collapse = "+")
#   fmlstring = paste("choice ~", fmlstring1, "| 0 | 0 | 0 |", fmlstring2)
#   fml2 = as.formula(fmlstring)
# }
# 
# # koliko segmenata?? staviti u Q = 
# lca_segments = designctx$rsegments
# 
# ## BUILD AND SAVE MODEL
# library(gmnl)
# lcm = gmnl(fml2, data = cbc.mlogit, model = "lc", Q = lca_segments, panel = TRUE, method = 'bhhh') # cbc.mlogit je definiran kod mixed mnl gore
# saveRDS(lcm, lcmodelfile)
# 
# ## LOAD MODEL
# lcm = readRDS(lcmodelfile)
# #sm.lc = summary(lcm)$CoefTable
# sm.lc = lcm$coefficients
# sm.lc
# 
# ### GMNL LCA <<<


### >>> STAN MC LCA
lca_segments = designctx$rsegments
### STAN MC LCA <<<

```

Taking into account different factors (demographic or other), we can divide the respondents into segments and then model how each segment (instead of average or individual) values the products. The number of segments we believe exists in the respondents set must be given in advance as a parameter. Assuming that there are `r lca_segments` different respondents segments we build the multinomial logit model with LCA (Latent Class Analysis) and get the following summary:


```{r echo= FALSE, warning = FALSE, message = FALSE}
### >>> GMNL LCA
# print(sm.lc[, c("Estimate", "Std. Error")])
### GMNL LCA <<<

### >>> STAN MC LCA
stan.mc.lca.betas.mean
### STAN MC LCA <<<
```



```{r echo= FALSE, warning = FALSE, message = FALSE}

nivo_znacajnosti_za_razliku = .5

# ### GMNL LCA >>>
# 
# cfs = coefficients(lcm)
# cfs_s = list()
# r = lapply(1:lca_segments, function(s) {
#   wh = which(grepl(paste("class.", s, ".", sep = ""), names(cfs)))
#   cfs_s[[s]] <<- cfs[wh]
#   names(cfs_s[[s]]) <<- substring(names(cfs_s[[s]]), 9) # 9 je prvi character nakon class.1.
# })
# cfs_s_diffs = names(cfs_s[[1]][which(abs(cfs_s[[1]]-cfs_s[[2]]) > nivo_znacajnosti_za_razliku)])
# 
# # source('lc_helpers.R')
# # plot_ci_lc(lcm)
# # shares(lcm)
# 
# ### GMNL LCA <<<

```

```{r echo= FALSE, warning = FALSE, message = FALSE}
### STAN MC LCA >>>

cfs_s_diffs = c()
r = lapply(1:(lca_segments-1), function(s) {
  cfs_s_diffs <<- unique(c(cfs_s_diffs, names(which(abs(stan.mc.lca.betas.mean[, s] - stan.mc.lca.betas.mean[, (s+1)]) > nivo_znacajnosti_za_razliku))))  
})

### STAN MC LCA <<<

```

We can see that the segments differ (for more than `r nivo_znacajnosti_za_razliku`) in valuation of e.g. *`r cfs_s_diffs`*.

Since one of the goals of the segmentation is assigning each respondent to some segment (with the highest probability), let's see this result too (the numbers given are the numbers of the respondents belonging to a certain segment):


```{r echo= FALSE, warning = FALSE, message = FALSE}
# ### GMNL LCA >>>
# 
# # bi_juhapileca.juha <- effect.gmnl(lcm, par = "juhapileća.juha", effect = "ce")$mean
# # summary(bi_juhapileca.juha)
# # 
# # plot(lcm, par = "juhapileća.juha", effect = "ce", type = "density", col = "blue")
# # 
# # Conditional probabilitites
# pi_hat = lcm$Qir
# colnames(pi_hat) = paste("q =", 1:lca_segments)
# # segment (s najvećom vjerojatnošću) za svakog ispitanika
# segs = apply(pi_hat, 1, which.max)
# r = lapply(1:lca_segments, function(s) {
#   print(paste("Segment", s))
#   print(which(segs == s))
# })
# 
# ### GMNL LCA <<<
# #############################################################################################################

```

```{r echo= FALSE, warning = FALSE, message = FALSE}
### STAN MC LCA >>>

resp_coefs = apply(stan.mc.lca.theta, 2:3, mean)
resp_probs = apply(resp_coefs, 2, softmax)
# moramo maknuti 0 ili NA
r = lapply(1:ncol(resp_probs), function(c) {
  if ((all(resp_probs[, c]) == 0) | (any(is.na(resp_probs[, c])))) resp_probs[, c] <<- (1/lca_segments)
})
resp_segs = apply(resp_probs, 2, which.max)

r = lapply(1:lca_segments, function(s) {
  print(paste("Segment", s))
  print(which(resp_segs == s))
})

### STAN MC LCA <<<
#############################################################################################################

```


Since one of the segments is empty (no respondents in it), we conclude that the number of segments that we can identify with the given data and model is `r lca_segments-1`.

With the given segmentation we can e.g. create different products that will be the most interesting (best valued) by each segment, thus increasing buyers satisfaction and income/profit.


## TURF analysis

With an offer that contains more products we will increase the probability that at least some product will be sold. To calculate which products should be offered to achieve the highest number of sales, we can use TURF (total unduplicated reach and frequency) analysis.

```{r echo= FALSE, warning = FALSE, message = FALSE}
comb_size = 3 # size of the TURF combination that we will test
```

Let's assume that we want to put `r comb_size` products in an offer. For each combination/offer containing `r comb_size` products and for each respondent from our survey, we will calculate whether he would buy at least one product from the offer (reach - is he 'reached' with at least one product) and how many (if any) products from the offer would he buy (frequency - how many times is he 'reached' with the offer). The respondent's buying decision for a product is based on the probability of sales for that product compared with the other products in the offer and the basic threshold option marking the buying/not buying boundary.

Let's have a look at a few rows from the reach & frequency table for our products and offers (ordered decreasing by reach):

```{r echo= FALSE, warning = FALSE, message = FALSE}

colitems = make.names(designctx$items)
respids = unique(cbc.df$resp.id)
items_combs = combn(designctx$items, comb_size)

# combs_df = data.frame(respid = rep(respids, ncol(items_combs)))
# combinations = sapply(1:ncol(items_combs), function(itc) paste(items_combs[, itc], collapse = ", "))
# combs_df$combination = rep(combinations, each = length(respids))
# for (ci in colitems) combs_df[[ci]] = 0
# combs_df$reach = combs_df$freq = 0
# for (i in 1:ncol(items_combs)) {
#   comb_cols = colnames(combs_df) %in% make.names(items_combs[, i])
# #    combs_df[(1:length(respids))+(i-1)*length(respids), comb_cols] = 1
#   for (r in 1:length(respids)) {
#     ir_utils = calc.utilities.hb.zc(model = hb.post, data = data.frame(opcije = items_combs[, i]), resp_i = respids[r])
#     probs_reach = exp(ir_utils)/(sum(exp(ir_utils))+1) # ovo je ok ako imamo anchor, provjeriti još da li je ok ako ga nemamo
#     reached = sapply(probs_reach, function(prob_reach) {
#       sample(c(TRUE, FALSE), 1, replace = FALSE, prob = c(prob_reach, 1-prob_reach))
#     })
#     resp_comb_row = r+(i-1)*length(respids)
#     combs_df[resp_comb_row, comb_cols] = ifelse(reached, as.numeric(1), as.numeric(-1))
#     combs_df[resp_comb_row, "reach"] = as.numeric(any(reached))
#     combs_df[resp_comb_row, "freq"] = as.numeric(sum(reached))
#   }
#  if ((i %% 10) == 0) print(i)
# }
# saveRDS(combs_df, turffile)

combs_df = readRDS(turffile)

combs_out = combs_df %>% dplyr::group_by(combination) %>% dplyr::summarise(reach = sum(reach), freq = sum(freq))
combs_out = combs_out[order(combs_out$reach, decreasing = TRUE), ]

head(combs_out)

```

```{r echo= FALSE, warning = FALSE, message = FALSE}

# https://en.wikipedia.org/wiki/McNemar%27s_test
comb1_i = 1
comb2_i = 2
comb_diff = FALSE
while ((comb_diff == FALSE) & (comb2_i <= nrow(combs_out))) {
  comb1 = combs_out[comb1_i, ]$combination
  comb2 = combs_out[comb2_i, ]$combination
  comb1_positive = combs_df[(combs_df$combination == comb1) & (combs_df$reach == 1), ]$respid
  comb1_negative = combs_df[(combs_df$combination == comb1) & (combs_df$reach == 0), ]$respid
  comb2_positive = combs_df[(combs_df$combination == comb2) & (combs_df$reach == 1), ]$respid
  comb2_negative = combs_df[(combs_df$combination == comb2) & (combs_df$reach == 0), ]$respid
  mcnemar_a = length(dplyr::intersect(comb1_positive, comb2_positive)) # both positive
  mcnemar_b = length(dplyr::intersect(comb1_positive, comb2_negative)) # 1 positive, 2 negative
  mcnemar_c = length(dplyr::intersect(comb1_negative, comb2_positive)) # 1 negative, 2 positive
  mcnemar_d = length(dplyr::intersect(comb1_negative, comb2_negative)) # both negative
  mcnemar_x = matrix(c(mcnemar_a, mcnemar_b, mcnemar_c, mcnemar_d), 
                     nrow = 2,
                     byrow = TRUE,
                     dimnames = list("1st Comb" = c("Reached", "Not Reached"), "2nd Comb" = c("Reached", "Not Reached")))
  mcnemar_out = mcnemar.test(mcnemar_x)
  # 0 hipoteza je da nema stvarne razlike između kombinacija, alternativna je da razlika postoji
  # znači ako je p-value veća od npr. 0.05, zadržavamo 0 hipotezu, ako je manja, odbacujemo 0 hipotezu u korist alternativne
  comb_diff = (mcnemar_out$p.value < 0.05)
  comb2_i = comb2_i + 1
}

# if (comb2_i > nrow(combs_out)) {
#   print("All combinations are statistically equal.")
# } else {
#   print(paste("First ", comb2_i-1, " combinations are statistically equal.", sep = ""))
#   combs_out[1:(comb2_i-1), ]
# }

```


```{r echo= FALSE, warning = FALSE, message = FALSE}

# turf_data = data.frame(respid = unique(cbc.df$resp.id), wgt = rep(1, length(unique(cbc.df$resp.id))))
# colitems = make.names(designctx$items)
# for (i in colitems) {
#   turf_data[[i]] = 0
# }
# 
# zero_buyers = c()
# for (r in turf_data$respid) {
#   for (i in designctx$items) {
#     ir_util = calc.utilities.hb.zc(model = hb.post, data = data.frame(opcije = i), resp_i = r)
#     prob_reach = exp(ir_util)/(exp(ir_util)+(designctx$nalternatives-1)) # ovo je ok ako imamo anchor, provjeriti još da li je ok ako ga nemamo
#     reached = sample(c(TRUE, FALSE), 1, replace = FALSE, prob = c(prob_reach, 1-prob_reach))
#     turf_data[turf_data$respid == r, make.names(i)] = as.numeric(reached)
#   }
#   if (all(turf_data[r, colitems] == 0)) zero_buyers = c(zero_buyers, r)
# }
# 
# library(turfR)
# # maknuti one koji ništa ne kupuju
# turf_data = turf_data[!(turf_data$respid %in% zero_buyers), ]
# comb_sizes = 2:6
# keep = 3
# turf_out = turf(turf_data, ncol(turf_data)-2, comb_sizes, 
#                 depth = 1, keep = keep, mc = FALSE, nsims = 10000, psims = NULL, sort = "d")
# 
# turf_m = NULL
# for (ci in 1:length(comb_sizes)) {
#   turf_i = turf_out$turf[[ci]]
#   combination = c()
#   for (ri in 1:keep) {
#     combination = c(combination, paste(designctx$items[which(turf_i[ri, -c(1:3)] == 1)], collapse = ", "))
#   }
#   turf_df = data.frame(comb_size = comb_sizes[ci], combination = combination, reach = turf_i$rchX, freq = turf_i$frqX)
#   if (is.null(turf_m)) {
#     turf_m = turf_df
#   } else {
#     turf_m = rbind(turf_m, turf_df)
#   }
# }
# 
# turf_m

```

We can see that the offer `r as.character(combs_out[1, "combination"])` would reach (at least one product would be bought by) the most respondents, `r as.numeric(combs_out[1, "reach"])` of them, which is `r round(as.numeric(combs_out[1, "reach"])/length(respids), 2)*100`% of the total number of respondents, with a total of `r as.numeric(combs_out[1, "freq"])` (potential) sales.

This way we can build an offer of products that will maximize the sales.


## Conclusion

**MaxDiff analysis is a powerful tool for gaining knowledge of your customers and their values and creating the best products for them. This approach always leads to the business improvement, higher incomes and higher customer satisfaction.**

For more information contact [**danijel.kopcinovic@itmarket.hr**](mailto:danijel.kopcinovic@itmarket.hr).

